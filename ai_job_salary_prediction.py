# -*- coding: utf-8 -*-
"""Ai job salary prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h4nWugN_P--7FeuVjx9RIJ6b2PxKFqGo
"""



import pandas as pd

# Replace 'path/to/your/dataset.csv' with the actual path to your dataset
try:
    df = pd.read_csv("/ai_job_dataset.csv")
    print("Dataset loaded successfully!")
    display(df.head())
except FileNotFoundError:
    print("Error: Dataset file not found. Please update the path to your dataset.")
except Exception as e:
    print(f"An error occurred: {e}")

"""## Data Cleaning and Preprocessing

Before we start analyzing the data, it's essential to clean and preprocess it. This involves handling missing values, dealing with duplicates, and ensuring the data types are appropriate for analysis.

### Check for Missing Values
"""

# Check for missing values
print("Missing values before cleaning:")
display(df.isnull().sum())

"""Depending on the number and distribution of missing values, you might choose to:
- Drop rows or columns with missing values.
- Impute missing values using techniques like mean, median, mode, or more advanced methods.

### Handle Duplicate Rows
"""

# Check for duplicate rows
print("\nNumber of duplicate rows before cleaning:")
display(df.duplicated().sum())

# Drop duplicate rows (if any)
df.drop_duplicates(inplace=True)
print("\nNumber of duplicate rows after cleaning:")
display(df.duplicated().sum())

"""### Check Data Types"""

# Check data types
print("\nData types:")
display(df.dtypes)

"""Inspect the data types and convert columns to the appropriate types if necessary (e.g., convert object type to datetime for date columns).

## Exploratory Data Analysis (EDA)

Exploratory Data Analysis (EDA) is a critical step to understand the data's main characteristics, identify patterns, detect anomalies, and test hypotheses.

### Descriptive Statistics
"""

# Display descriptive statistics for numerical columns
print("Descriptive statistics for numerical columns:")
display(df.describe())

# Display descriptive statistics for categorical columns
print("\nDescriptive statistics for categorical columns:")
display(df.describe(include='object'))

"""### Data Visualization

Let's create some visualizations to explore the data.

#### Distribution of Salaries
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df['salary_usd'], kde=True)
plt.title('Distribution of Salaries (USD)')
plt.xlabel('Salary (USD)')
plt.ylabel('Frequency')
plt.show()

"""#### Job Titles Distribution (Top 10)"""

plt.figure(figsize=(12, 6))
df['job_title'].value_counts().nlargest(10).plot(kind='bar')
plt.title('Top 10 Most Frequent Job Titles')
plt.xlabel('Job Title')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""#### Relationship between Experience Level and Salary"""

plt.figure(figsize=(10, 6))
sns.boxplot(x='experience_level', y='salary_usd', data=df, order=['EN', 'MI', 'SE', 'EX'])
plt.title('Salary Distribution by Experience Level')
plt.xlabel('Experience Level')
plt.ylabel('Salary (USD)')
plt.show()

"""### Feature Engineering - Extracting Features from Dates"""

# Convert date columns to datetime objects
df['posting_date'] = pd.to_datetime(df['posting_date'])
df['application_deadline'] = pd.to_datetime(df['application_deadline'])

# Extract year, month, and day from posting_date
df['posting_year'] = df['posting_date'].dt.year
df['posting_month'] = df['posting_date'].dt.month
df['posting_day'] = df['posting_date'].dt.day

# Extract year, month, and day from application_deadline
df['deadline_year'] = df['application_deadline'].dt.year
df['deadline_month'] = df['application_deadline'].dt.month
df['deadline_day'] = df['application_deadline'].dt.day

# Calculate the duration the job is open (in days)
df['posting_duration'] = (df['application_deadline'] - df['posting_date']).dt.days

print("New date-based features created.")
display(df[['posting_date', 'application_deadline', 'posting_year', 'posting_month', 'posting_day', 'deadline_year', 'deadline_month', 'deadline_day', 'posting_duration']].head())

"""### Prepare Data for Modeling (with new features)"""

from sklearn.model_selection import train_test_split

# Define features (X) and target variable (y) with the updated DataFrame
# We need to exclude the original date columns as well as the target variable
X = df.drop(['salary_usd', 'posting_date', 'application_deadline'], axis=1)
y = df['salary_usd']

# Handle categorical features: One-Hot Encoding
X = pd.get_dummies(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data successfully prepared and split with new features:")
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""### Re-train the Model"""

from sklearn.linear_model import LinearRegression

# Initialize and train the Linear Regression model with the updated data
model = LinearRegression()
model.fit(X_train, y_train)

print("Linear Regression model re-trained successfully with new features!")

"""### Re-evaluate the Model"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Make predictions on the test data using the re-trained model
y_pred_re_trained = model.predict(X_test)

# Evaluate the re-trained model
mae_re_trained = mean_absolute_error(y_test, y_pred_re_trained)
mse_re_trained = mean_squared_error(y_test, y_pred_re_trained)
rmse_re_trained = np.sqrt(mse_re_trained)
r2_re_trained = r2_score(y_test, y_pred_re_trained)

print("Re-trained Model Evaluation Results (with new features):")
print(f"Mean Absolute Error (MAE): {mae_re_trained:.2f}")
print(f"Mean Squared Error (MSE): {mse_re_trained:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_re_trained:.2f}")
print(f"R-squared (R2): {r2_re_trained:.2f}")

"""### Model Interpretation - Coefficients"""

# Get the coefficients and feature names
coefficients = model.coef_
feature_names = X_train.columns

# Create a pandas Series to easily view coefficients with their corresponding feature names
coef_series = pd.Series(coefficients, index=feature_names)

# Display the coefficients, sorted by their absolute value to see the most impactful features
print("Model Coefficients (sorted by absolute value):")
display(coef_series.abs().sort_values(ascending=False))

# You can also display the coefficients without sorting to see the sign (positive or negative impact)
print("\nModel Coefficients (original order):")
display(coef_series)

"""### Visualizing Model Results"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_re_trained, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2) # Plotting the ideal line
plt.xlabel('Actual Salary (USD)')
plt.ylabel('Predicted Salary (USD)')
plt.title('Actual vs. Predicted Salary (with new features)')
plt.show()

"""## Trying a Random Forest Regressor Model

Let's train a Random Forest Regressor to see if it improves our salary predictions.
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Initialize the Random Forest Regressor model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42) # n_estimators can be tuned

# Train the model
print("Training Random Forest Regressor model...")
rf_model.fit(X_train, y_train)
print("Random Forest Regressor model trained successfully!")

# Make predictions on the test data
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print("\nRandom Forest Regressor Model Evaluation Results:")
print(f"Mean Absolute Error (MAE): {mae_rf:.2f}")
print(f"Mean Squared Error (MSE): {mse_rf:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_rf:.2f}")
print(f"R-squared (R2): {r2_rf:.2f}")

"""### Visualizing Random Forest Model Results"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_rf, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2) # Plotting the ideal line
plt.xlabel('Actual Salary (USD)')
plt.ylabel('Predicted Salary (USD)')
plt.title('Actual vs. Predicted Salary (Random Forest)')
plt.show()

"""Here's a summary of what we've found:

* Data Overview: The dataset provides information on various aspects of AI jobs, including salary, job title, experience level, company location and size, required skills, education, years of experience, and more. The data was relatively clean with no missing values or duplicates.
* Exploratory Data Analysis: We observed the distribution of salaries, the frequency of different job titles, and the relationship between experience level and salary. As expected, salaries tend to increase with experience level.
* Feature Engineering: We successfully extracted new features from the date columns, such as posting year, month, day, and the duration a job is posted.
* Model Performance:
       1. We trained a Linear Regression model which provided a reasonable baseline, explaining about 85% of the variance in salaries after including the engineered date features. However, the actual vs. predicted plot suggested some limitations, particularly at higher salary ranges.
       2. We then trained a Random Forest Regressor model, which showed improved performance compared to the Linear Regression model, with lower error metrics (MAE and RMSE) and a higher R-squared value (0.88). This suggests that the Random Forest model is better at capturing the patterns in this dataset for salary prediction.
* Model Comparison: The Random Forest Regressor model is the better-performing model between the two we tried based on the evaluation metrics.
Overall Conclusion:

Using the provided dataset, we can build models that can predict AI job salaries with a reasonable degree of accuracy. The Random Forest Regressor model performed better than the Linear Regression model, indicating that non-linear relationships and interactions between features are likely important in determining AI job salaries. Features derived from various job attributes and potentially from the posting dates contribute to these predictions.

While the Random Forest model shows good performance (R-squared of 0.88), there is still room for improvement, as indicated by the MAE and RMSE values. Further steps could involve hyperparameter tuning of the Random Forest model, exploring other advanced models, or conducting more in-depth feature analysis or engineering.

This project demonstrates a typical workflow for tackling a regression problem using machine learning.






"""